{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1599867271378",
      "display_name": "Python 3.8.5 64-bit ('pycuda': venv)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGQQYSQc-4YE"
      },
      "source": [
        "# Demo 1: An Introduction to PyCUDA\n",
        "## EECS E4750: Heterogenous Computing for Signal and Data Computing\n",
        "Fall 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmM-ew6Z-4YG"
      },
      "source": [
        "## Preface\n",
        "\n",
        "Welcome to E4750! In this Jupyter Notebook, you will explore what may very well be your first encounter with CUDA programming, through the popular Python wrapper - **PyCUDA**.\n",
        "\n",
        "While I am showcasing this demo through the use of a Jupyter Notebook, please keep in mind that assignments and even future recitations will eschew these - we will work exclusively with executable python scripts, not jupyter notebooks. Nevertheless, this and the PyOpenCL demo will be available on the course repo if you need to revisit either."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAGCjiE4-4YG"
      },
      "source": [
        "## The CUDA Device and APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEgjAJGE-4YG"
      },
      "source": [
        "### CUDA Initialization\n",
        "The first step is obviously to import the PyCUDA wrapper. The `pycuda.driver.init` method initializes CUDA. In all cases, this must always be the first step. Alternatively, you could try `pycuda.driver.autoinit`, which performs initialization as well as context creation.\n",
        "\n",
        "### CUDA Runtime and Driver APIs\n",
        "\n",
        "There are two APIs associated with what one collectively refers to as CUDA. These are the CUDA runtime and CUDA driver APIs. As the [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/cuda-driver-api/driver-vs-runtime-api.html#driver-vs-runtime-api) notes, the driver and runtime are quite similar and can infact be used interchangeably, although there are some noteworthy differences that separate the two.\n",
        "\n",
        "The runtime API provides some ease with implicit initialization, context management and module management. The driver API on the other hand, provides finer control over contexts and module loading. With the runtime, all kernels are automatically loaded during initialization and remain loaded as long as the program using those kernels is running. With the driver API, it is possible to retain modules on an as-needed-basis, and also to dynamically reload modules without re-initialization.\n",
        "\n",
        "The CUDA runtime API is a wrapper of the CUDA driver API.\n",
        "\n",
        "### Kernels\n",
        "\n",
        "CUDA C/C++ extends C/C++ by allowing one to define special functions, called *kernels*. When called, these are executed *N* times in parallel by *N* different CUDA threads, unlike normal host code functions, which would only be executed once upon being called.\n",
        "\n",
        "### Contexts\n",
        "\n",
        "Contexts can be thought of as the bridge between your host (loosely, the CPU) and the CUDA Device (more specifically, the Runtime API). All communication with the GPU takes place within and through the context. The context holds all management data to control and use the CUDA Device.; For example, it keeps track of allocated memory (because you do not want to run out of it or allocate more than physically available), the loaded modules that contain device code (i.e. kernels), and the mapping between host (CPU) and GPU memory.\n",
        "\n",
        "Why is this necessary? GPU device resources are actually scarce (indeed, the raw power of a CPU is usually greater than a GPU) and one can never have enough. Actual abundance in numbers of CUDA cores is not relevant if they are inaccessible. Therefore, you can think of the Context as a life-cycle manager - it manages the existence of objects in typical CUDA programs, such as:\n",
        "    * memory buffers\n",
        "    * modules\n",
        "    * functions\n",
        "    * CUDA streams\n",
        "    * events\n",
        "    * and more.\n",
        "\n",
        "The driver API lets you manage contexts, while these are not exposed to the runtime API. Instead, the runtime API dynamically decides which context to use for a process. There can only be one active context on the device at a time, and the objects within it are destroyed with the context when ended, so they are usable only within the context in which they were created.\n",
        "\n",
        "All this to say, a single context is created and used for when an on-device program is run. Since these programs will constitute one or more kernels, the key takeaway is that those specific kernels can be used only in the context they were loaded through. Once the context is gone, the kernels/program would need to be built and sourced again for the next context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEzO25il-4YH"
      },
      "source": [
        "\n",
        "For simplicity, let's first import pycuda and interact with the API to fetch some information about the CUDA device and the driver itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tj3VEpa-4YH"
      },
      "outputs": [],
      "source": [
        "import pycuda\n",
        "import pycuda.driver as cuda\n",
        "\n",
        "cuda.init()\n",
        "#cuda.autoinit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHuEktO9-4YI"
      },
      "source": [
        "Let's check the number of CUDA devices visible to the driver:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FBxS60P3-4YI",
        "outputId": "e545647a-76ce-4c25-b958-a157879feedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of CUDA devices available:  1\n"
        }
      ],
      "source": [
        "print(\"Number of CUDA devices available: \", cuda.Device.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUYyPA9y-4YI"
      },
      "source": [
        "One! This is unsurprising, since I am running this on a laptop with just one Nvidia GPU. If you ran this on a multi-GPU machine (lucky you!), you would see a different count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "AdoBlDUm-4YJ",
        "outputId": "248589c8-d480-4efe-d63b-1a05f89b78d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device Name: GeForce RTX 2070\nCompute Capability: 7.5\nTotal Device Memory: 7982 megabytes\n"
        }
      ],
      "source": [
        "my_device = cuda.Device(0)\n",
        "\n",
        "# cc: compute capability\n",
        "cc = float('%d.%d' % my_device.compute_capability())\n",
        "print('Device Name: {}'.format(my_device.name()))\n",
        "print('Compute Capability: {}'.format(cc))\n",
        "print('Total Device Memory: {} megabytes'.format(my_device.total_memory()//1024**2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bYeM9Svi-4YJ",
        "outputId": "a1c3d93b-1881-49db-969f-180421704285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ASYNC_ENGINE_COUNT: 3\n\t CAN_MAP_HOST_MEMORY: 1\n\t CLOCK_RATE: 1440000\n\t COMPUTE_CAPABILITY_MAJOR: 7\n\t COMPUTE_CAPABILITY_MINOR: 5\n\t COMPUTE_MODE: DEFAULT\n\t CONCURRENT_KERNELS: 1\n\t ECC_ENABLED: 0\n\t GLOBAL_L1_CACHE_SUPPORTED: 1\n\t GLOBAL_MEMORY_BUS_WIDTH: 256\n\t GPU_OVERLAP: 1\n\t INTEGRATED: 0\n\t KERNEL_EXEC_TIMEOUT: 1\n\t L2_CACHE_SIZE: 4194304\n\t LOCAL_L1_CACHE_SUPPORTED: 1\n\t MANAGED_MEMORY: 1\n\t MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n\t MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n\t MAXIMUM_SURFACE1D_WIDTH: 32768\n\t MAXIMUM_SURFACE2D_HEIGHT: 65536\n\t MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n\t MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n\t MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n\t MAXIMUM_SURFACE2D_WIDTH: 131072\n\t MAXIMUM_SURFACE3D_DEPTH: 16384\n\t MAXIMUM_SURFACE3D_HEIGHT: 16384\n\t MAXIMUM_SURFACE3D_WIDTH: 16384\n\t MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n\t MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n\t MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n\t MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n\t MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n\t MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n\t MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n\t MAXIMUM_TEXTURE1D_WIDTH: 131072\n\t MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n\t MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n\t MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n\t MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n\t MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n\t MAXIMUM_TEXTURE2D_HEIGHT: 65536\n\t MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n\t MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n\t MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n\t MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n\t MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n\t MAXIMUM_TEXTURE2D_WIDTH: 131072\n\t MAXIMUM_TEXTURE3D_DEPTH: 16384\n\t MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n\t MAXIMUM_TEXTURE3D_HEIGHT: 16384\n\t MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n\t MAXIMUM_TEXTURE3D_WIDTH: 16384\n\t MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n\t MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n\t MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n\t MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n\t MAX_BLOCK_DIM_X: 1024\n\t MAX_BLOCK_DIM_Y: 1024\n\t MAX_BLOCK_DIM_Z: 64\n\t MAX_GRID_DIM_X: 2147483647\n\t MAX_GRID_DIM_Y: 65535\n\t MAX_GRID_DIM_Z: 65535\n\t MAX_PITCH: 2147483647\n\t MAX_REGISTERS_PER_BLOCK: 65536\n\t MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n\t MAX_SHARED_MEMORY_PER_BLOCK: 49152\n\t MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n\t MAX_THREADS_PER_BLOCK: 1024\n\t MAX_THREADS_PER_MULTIPROCESSOR: 1024\n\t MEMORY_CLOCK_RATE: 7001000\n\t MULTIPROCESSOR_COUNT: 36\n\t MULTI_GPU_BOARD: 0\n\t MULTI_GPU_BOARD_GROUP_ID: 0\n\t PCI_BUS_ID: 1\n\t PCI_DEVICE_ID: 0\n\t PCI_DOMAIN_ID: 0\n\t STREAM_PRIORITIES_SUPPORTED: 1\n\t SURFACE_ALIGNMENT: 512\n\t TCC_DRIVER: 0\n\t TEXTURE_ALIGNMENT: 512\n\t TEXTURE_PITCH_ALIGNMENT: 32\n\t TOTAL_CONSTANT_MEMORY: 65536\n\t UNIFIED_ADDRESSING: 1\n\t WARP_SIZE: 32\n"
        }
      ],
      "source": [
        "# In CUDA/C, deviceQuery will display a bit more\n",
        "# detail. To see these using PyCUDA, we will set up\n",
        "# a python dictionary to index the values\n",
        "# device.get_attributes will provide.\n",
        "\n",
        "my_dev_attributes_tuples = my_device.get_attributes().items()\n",
        "mydev_attributes = {}\n",
        "\n",
        "for key, value in my_dev_attributes_tuples:\n",
        "    mydev_attributes[str(key)] = value\n",
        "\n",
        "for k in mydev_attributes.keys():\n",
        "    print('\\t {}: {}'.format(k, mydev_attributes[k]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iKPtZus-4YJ"
      },
      "source": [
        "## Demo PyCUDA program\n",
        "\n",
        "Let us explore a simple PyCUDA program that doubles the elements of an input numpy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bwrvRba-4YJ"
      },
      "outputs": [],
      "source": [
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zbJYKkL-4YJ"
      },
      "outputs": [],
      "source": [
        "# Create a 4x4 2D numpy array\n",
        "\n",
        "a = np.random.randn(4,4)\n",
        "a = a.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80gWiASL-4YJ"
      },
      "source": [
        "### Memory Allocation and Host-to-Device Transfer\n",
        "\n",
        "This array `a` is currently on the host memory, so no GPU operation can be performed on it. (To be pedantic, PyCUDA will still let you do this but if your host program were written in C/C++, your kernel won't launch - since there is no variable in the device memory to operate on. You will investigate the ramifications of this requirement in Assignment-1.)\n",
        "\n",
        "So, the first step is to allocate *exactly* amount of memory in device memory that the FP32 variable `a` occupies in host memory. Above, we forced the numpy array `a` to be of the data type `numpy.float32`. This is because GPUs, unlike CPUs, do not support 64 bit floating point precision - only 32. So, for variables created on the host, this requirement must be enforced to avoid trouble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtG4iuRB-4YK"
      },
      "outputs": [],
      "source": [
        "a_gpu = cuda.mem_alloc(a.size * a.dtype.itemsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOi86OgC-4YK"
      },
      "source": [
        "Just allocating memory isn't enough - you need to actually copy the array to device memory from its host location to the allocated memory on the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s89U4kmF-4YK"
      },
      "outputs": [],
      "source": [
        "cuda.memcpy_htod(a_gpu, a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6EG9Pwm-4YK"
      },
      "source": [
        "### Kernel Code\n",
        "\n",
        "Next, we need a CUDA kernel that defines exactly what the GPU is to do with the array that we are placing in it's memory. With CUDA C/C++, this is quite simple. But since we're using PyCUDA, the process is a little different. While the majority of peripheral code you will encounter in assignments will by pythonic, the kernel code itself **must be written in C syntax**. Hence, we let the driver create a module for our kernel in CUDA source for the current context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ErpvUmv-4YK"
      },
      "outputs": [],
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "    __global__ void doublify(float *a)\n",
        "    {\n",
        "      int idx = threadIdx.x;\n",
        "      a[idx] *= 2;\n",
        "    }\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aE0FLiG-4YK"
      },
      "source": [
        "Here, `__global__` indicates the function is a CUDA kernel function â€“ called by the host and executed on the\n",
        "device, while `void` means that the kernel doesn't return anything.\n",
        "\n",
        "* `doublify`: Kernel function name\n",
        "* `float *a`: The kernel function arguments. `*a` because we're working with a pointer to the device memory.\n",
        "\n",
        "CUDA kernel execution is done in chunks, called thread-blocks/blocks. Each device has a specific maximum blocksize i.e. a maximum number of threads that can be run concurrentlyu for any block. You can additionally define a grid of multiple blocks. We will ignore this for now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm1MUaAE-4YK"
      },
      "source": [
        "### Thread Indexing\n",
        "\n",
        "Let's look closely at the second line of the kernel function.\n",
        "\n",
        "* `int idx = threadIdx.x + threadIdx.y*4;`\n",
        "\n",
        "This defines a unique thread id among all threads in a grid. What do those individual words mean?\n",
        "\n",
        "* Threadblocks are 2D - so they will have two dimensions along which to index.\n",
        "* `threadIdx.x`: specifies the x-index of a local thread id within a 2D thread block\n",
        "* `threadIdx.y`: specifies the y-index of a local thread id within a 2D thread block\n",
        "\n",
        "For this example we aren't looking at grids; but if we were, there is an additional detail to understand. Consider another example of 1D indexing (for a completely unrelated kind of kernel):\n",
        "`int i = blockDim.x * blockIdx.x + threadIdx.x;`\n",
        "\n",
        "* `blockDim`: gives the number of threads within each block (in the x-dimension in 1D case)\n",
        "* `blockIdx`: specifies which block the thread belongs to (within the grid of blocks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZDau65j-4YK"
      },
      "source": [
        "### Where's the for loop?\n",
        "\n",
        "So if you're wondering where the for loop is to define which index has to be worked on when, you're asking a pertinent question. Recall that kernels are executed *N* times in parallel by *N* different CUDA threads. The indexing we've defined controls how many threads are handled at once.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDZt9to7-4YK"
      },
      "source": [
        "### Launching the kernels\n",
        "\n",
        "The compiled kernel code can be called by using `mod.get_function`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UWFXAGL-4YK"
      },
      "outputs": [],
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "start = cuda.Event()\n",
        "end = cuda.Event()\n",
        "\n",
        "start.record()\n",
        "func(a_gpu, block=(4,4,1))\n",
        "end.record() # wait for event to finish\n",
        "\n",
        "# time event execution in milliseconds\n",
        "t = start.time_till(end)\n",
        "# blocksize=(4, 4, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZkX7xxl-4YL"
      },
      "source": [
        "Your block size determines in what order your input array is read by the kernel code. In this case, each block is entirely composed of the input array and nothing else. So, thread indexing was written the way we did:\n",
        "* `int idx = threadIdx.x + threadIdx.y*4;`\n",
        "\n",
        "Which means that the kernel executes the block it receives by treating it as a 1D block of 1D length elements. ([Read about row-major indexing](https://en.wikipedia.org/wiki/Row-_and_column-major_order))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEo6QQfn-4YL"
      },
      "source": [
        "### Fetching the result\n",
        "\n",
        "Once execution has concluded, the result must be retrieved and stored in host memory to be displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQirGUne-4YL"
      },
      "outputs": [],
      "source": [
        "a_doubled = np.empty_like(a)\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5NH-VLuz-4YL",
        "outputId": "287efee7-196f-4645-ada0-aff930e60c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Original array:  [[-0.16816713  0.70489305 -1.1215461  -0.43767273]\n [ 1.4034244  -1.6449759   0.5100117   0.8407693 ]\n [-3.494945   -0.7225162  -0.74454343 -1.0320127 ]\n [ 0.05584498  0.70544595 -1.1988835   2.0706177 ]]\nDoubled array:  [[-0.33633426  1.4097861  -2.2430923  -0.87534547]\n [ 1.4034244  -1.6449759   0.5100117   0.8407693 ]\n [-3.494945   -0.7225162  -0.74454343 -1.0320127 ]\n [ 0.05584498  0.70544595 -1.1988835   2.0706177 ]]\nExecution time: 0.9976639747619629  milliseconds.\n"
        }
      ],
      "source": [
        "print('Original array: ', a)\n",
        "print('Doubled array: ', a_doubled)\n",
        "print('Execution time:', t, ' milliseconds.')"
      ]
    }
  ]
}