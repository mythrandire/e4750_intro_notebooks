{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1599867254791",
      "display_name": "Python 3.8.5 64-bit ('pocl': venv)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5CmsyyV_EQF"
      },
      "source": [
        "# Demo 2: An Introduction to PyOpenCL\n",
        "## EECS E4750: Heterogenous Computing for Signal and Data Computing\n",
        "Fall 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icv2ZYeU_EQG"
      },
      "source": [
        "## Preface\n",
        "\n",
        "Welcome to E4750! In this Jupyter Notebook, you will explore what may very well be your first encounter with OpenCL programming, through the popular Python wrapper - **PyOpenCL**.\n",
        "\n",
        "While I am showcasing this demo through the use of a Jupyter Notebook, please keep in mind that assignments and even future recitations will eschew these - we will work exclusively with executable python scripts, not jupyter notebooks. Nevertheless, this and the PyCUDA demo will be available on the course repo if you need to revisit either."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjsUFMij_EQH"
      },
      "source": [
        "## About OpenCL\n",
        "\n",
        "Unlike CUDA, which is a proprietary API, OpenCL is a standard aimed broadly at every possible device/platform. As a result, multiple *implementations* of the OpenCL standard exist, which are provided by device vendors to support their platforms. So, Intel has its own implementation, as did AMD for its CPU/APUs. (Unfortunately, the highly anticipated Ryzen series don't seem to have OpenCL support, which is a real shame. The Radeon graphics products do still support OpenCL through ROCm, though). Nvidia too, ships OpenCL support with their CUDA toolkit.\n",
        "\n",
        "While originally created by Apple, maintenance of the standard has long since moved into the care of the Khronos Group, a non-profit consortium including most big-name usual suspects in the computing world, along with several academic institutions. Apple meanwhile has shifted to an in-house proprietary API called Metal. Do not be fooled into thinking that OpenCL is therefore irrelevant, though! While it is true that resources for learning OpenCL are harder to find than for CUDA, being Open Source has its advantages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F4CvFjE_EQH"
      },
      "source": [
        "## PyOpenCL\n",
        "\n",
        "Just like PyCUDA, PyOpenCL is a Python wrapper for OpenCL, with the same scope (full support) and similar usage.\n",
        "\n",
        "## Platform, Device and Context\n",
        "\n",
        "Let's begin by exploring the OpenCL platform and the Context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VFsePuiC_EQH",
        "outputId": "d64d869f-3c87-48cf-da06-5ab8793d6a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device name:  GeForce RTX 2070\nOpenCL device/platform name: NVIDIA CUDA\nGlobal Memory Size:  7982  megabytes\n"
        }
      ],
      "source": [
        "import pyopencl as cl\n",
        "\n",
        "# Create a context\n",
        "ctx = cl.create_some_context()\n",
        "\n",
        "# Query and print available device(s)\n",
        "device = ctx.devices[0]\n",
        "print('Device name: ', device.name)\n",
        "\n",
        "# Query platform and print\n",
        "platforms = cl.get_platforms()\n",
        "print('OpenCL device/platform name:', platforms[0].name)\n",
        "\n",
        "# Device Global Memory\n",
        "print('Global Memory Size: ', device.global_mem_size//1024**2, ' megabytes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W574vmjZ_EQI"
      },
      "source": [
        "So the platform refers to the vendor-specific OpenCL implementation, while contexts are used by the OpenCL runtime for managing objects such as command queues (the object that allows you to send commands to the device). These would also include memory, program, and kernel objects. A context facilitates kernels execution on one or more devices specified in that context.\n",
        "\n",
        "Let's now check the number of Streaming Multiprocessors (SMs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_0uWMzRq_EQI",
        "outputId": "b126a9f0-e4f3-4ad7-e9b5-15195040c548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SM count:  36\n"
        }
      ],
      "source": [
        "print('SM count: ', device.max_compute_units)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfburMo__EQI"
      },
      "source": [
        "### CUDA cores vs. Streaming Multiprocessors\n",
        "\n",
        "SMs and CUDA cores are not the same. CUDA cores are more prominently showcased and therefore these are what you're familiar with. The GPU architecture is composed of a certain number of Streaming Multiprocessors (SM). In the case of the card in use above, there are 36 SMs. For this specific device architecture, Nvidia includes 64 CUDA cores per SM. While you can easily Google these details for the Nvidia GPU that you'll use in a Google Cloud VM, it'll be a good exercise to try these device queries and discover these details yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-PfZPAR_EQI"
      },
      "source": [
        "## Demo PyOpenCL Program\n",
        "\n",
        "A basic OpenCL program structure has 3 levels:\n",
        "* Platform\n",
        "    * query platform\n",
        "    * query compute devices\n",
        "    * create contexts\n",
        "\n",
        "* Runtime\n",
        "    * create memory objects associate the contexts\n",
        "    * compile and create kernel program objects\n",
        "    * issue commands to command queue\n",
        "    * synchronization of commands\n",
        "    * clean up OpenCL resources\n",
        "\n",
        "* Kernels/Language layer\n",
        "    * OpenCL C Kernel Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBzkmRnB_EQJ"
      },
      "source": [
        "Let's look at an example similar to the PyCUDA demo - to double a 2D array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReIULWVN_EQJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "a_np = np.random.rand(5).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9STwPyp_EQJ"
      },
      "source": [
        "Once again, the device FP precision limits the input variable to `float32`.\n",
        "\n",
        "Next, create a context.\n",
        "\n",
        "### Command Queue\n",
        "\n",
        "The Command Queue highlights the advantage that OpenCL holds over CUDA. Unlike with CUDA, OpenCL contexts can be created for multiple devices. Command queues contain instructions to inform which of the devices of your Context (that is, the group of Devices you have chosen to use) is going to execute a particular command and also how it is going to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCGRPwaa_EQJ"
      },
      "outputs": [],
      "source": [
        "ctx = cl.create_some_context()\n",
        "queue = cl.CommandQueue(ctx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIBwyXrg_EQJ"
      },
      "source": [
        "The next step is to write the input array to a buffer object in device memory. You can read more about memory interactions in the [PyOpenCL documentation](https://documen.tician.de/pyopencl/runtime_memory.html). With the command below, you are writing the input array `a_np` into an on-device buffer with the memory flag `cl.mem_flags.COPY_HOST_PTR` - which tells the device to copy all values at the given address to device memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WsFxbCD_EQJ"
      },
      "outputs": [],
      "source": [
        "mf = cl.mem_flags\n",
        "a_g = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=a_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1sjxp4__EQJ"
      },
      "source": [
        "### Kernel Code\n",
        "\n",
        "You can define a kernel code and then build it using `cl.Program`. Below, the kernel code performs array doubling like in the PyCUDA tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud0YC47G_EQJ"
      },
      "outputs": [],
      "source": [
        "prg = cl.Program(ctx, \"\"\"\n",
        "__kernel void doublify(\n",
        "    __global const float *a_g, __global float *res_g)\n",
        "{\n",
        "  int gid = get_global_id(0);\n",
        "  res_g[gid] = a_g[gid]*2;\n",
        "}\n",
        "\"\"\").build()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6ESCNu_EQJ"
      },
      "source": [
        "The next steps would be to call the built kernel function and give it the parameters it needs to run. You also need to define an additional buffer to write the result into. So this is created with the `WRITE_ONLY` flag. The function call requires you to specify which queue to use. In our case, we've defined one context and corresponding command queue, so that will go here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TMB6otXO_EQK",
        "outputId": "f5858fd8-1c25-4f0b-8329-fc4574b80bb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<pyopencl._cl.Event at 0x7fc9ca1e5f90>"
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "res_g = cl.Buffer(ctx, mf.WRITE_ONLY, a_np.nbytes)\n",
        "prg.doublify(queue, a_np.shape, None, a_g, res_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzxz0uYE_EQK"
      },
      "source": [
        "### Copy result back to Host\n",
        "\n",
        "Finally, you have to copy the result from the device memory buffer back to host, which is of course done through the same command queue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "oxksRnfs_EQK",
        "outputId": "8c96b780-4644-4415-96c3-016d7ebab67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[1.1207193e+00 9.2828828e-01 1.7077771e+00 7.5119704e-01 1.4687362e-03]\n[1.1207193e+00 9.2828828e-01 1.7077771e+00 7.5119704e-01 1.4687362e-03]\n"
        }
      ],
      "source": [
        "res_np = np.empty_like(a_np)\n",
        "cl.enqueue_copy(queue, res_np, res_g)\n",
        "\n",
        "# Check on CPU with Numpy:\n",
        "print(res_np)\n",
        "print(a_np*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeR914Wb_EQK"
      },
      "source": [
        "### Error Handling with `try/except` in Python\n",
        "\n",
        "Here's an example on how to properly check for errors and handle specific errors. We'll use `try` and `except` to check if the results from the OpenCL kernel execution match standard numpy doubling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Z9nB-9vy_EQK",
        "outputId": "c051eaf7-e30f-4f93-f0c3-58537908143b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Checkpoint: Do python and opencl result match? Checking...\nResults match!\n"
        }
      ],
      "source": [
        "# Error check\n",
        "try:\n",
        "    print(\"Checkpoint: Do python and opencl result match? Checking...\")\n",
        "    assert (res_np-a_np).all()\n",
        "    baddouble = False   # results agree mutually\n",
        "except AssertionError:\n",
        "    print(\"Checkpoint failed: Python and opencl kernel result do not match. Try Again!\")\n",
        "\n",
        "    # perform some additional operation if assertion error occurs\n",
        "    # operation in case of exception goes here\n",
        "\n",
        "# if the interpreter gets this far, the error check passed!\n",
        "print('Results match!')"
      ]
    }
  ]
}